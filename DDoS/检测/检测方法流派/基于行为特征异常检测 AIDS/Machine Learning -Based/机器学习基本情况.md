# 基本情况

机器学习的工作原理
机器学习使用两种类型的技术：监督学习，它在已知的输入和输出数据上训练模型，以便它可以预测未来的输出；无监督学习，它在输入数据中发现隐藏的模式或内在结构。
机器学习技术：无监督学习（聚类）和监督学习（分类和回归）。

# 监督学习

监督式机器学习会构建一个模型，在存在不确定性的情况下根据证据做出预测。监督式学习算法采用一组已知的输入数据和对数据的已知响应（输出），并训练模型以对新数据的响应生成合理的预测。如果您有要预测的输出的已知数据，请使用监督式学习。
监督学习使用分类和回归技术来开发 机器学习模型。
分类技术可以预测离散响应 — 例如，一封电子邮件是真实的还是垃圾邮件，或者一个肿瘤是癌性的还是良性的。分类模型将输入数据分为几类。典型应用包括医学成像、语音识别和信用评分。
如果您的数据可以标记、分类或分成特定的组或类别，请使用分类。例如，手写识别应用程序使用分类来识别字母和数字。在图像处理和计算机视觉中，无监督模式识别技术用于对象检测和图像分割。执行分类的最常见算法可以在这里找到。

回归技术可以预测连续响应，例如难以测量的物理量，如电池充电状态、电网电力负荷或金融资产价格。典型应用包括虚拟传感、电力负荷预测和算法交易。
如果您要处理数据范围或响应性质为实数（例如温度或设备发生故障的时间），请使用回归技术。执行回归的最常见算法可在此处找到。
监督机器学习了解如何使用监督机器学习来训练模型，将输入映射到输出并预测新输入的响应。

## ensemble
有效提升精度
### bagging
将数据集分为小样本组分类，结果由平均/投票决定
### boosting
使用多个分类器，结果以投票呈现
# 无监督学习

无监督学习可以发现数据中的隐藏模式或内在结构。它用于从由没有标记响应的输入数据组成的数据集中得出推论。
聚类 是最常见的无监督学习技术。它用于探索性数据分析，以查找数据中的隐藏模式或分组。聚类分析的应用包括基因序列分析、市场研究和对象识别。
例如，如果一家手机公司想要优化他们建造手机信号塔的位置，他们可以使用机器学习来估计依赖其信号塔的人群数量。一部手机一次只能与一座信号塔通话，因此该团队使用聚类算法来设计手机信号塔的最佳位置，以优化其客户群体或群体的信号接收。执行聚类的最常见算法可以在这里找到。

聚类是一种机器学习方法，可以在无需监督的情况下在数据中发现隐藏的模式。
聚类可以发现数据中的隐藏模式。
无监督机器学习了解无监督机器学习的概述，该方法在没有标记响应的数据集中寻找模式。当您不确定数据包含哪些信息时，这种方法可让您探索数据。

# 如何决定使用哪种机器学习算法？

选择正确的算法似乎很困难——有几十种监督和无监督的机器学习算法，每种算法都采用不同的学习方法。
没有最好的方法或一刀切的方法。找到正确的算法在一定程度上只是反复试验——即使是经验丰富的数据科学家如果不尝试，也无法判断算法是否有效。但算法的选择还取决于你正在处理的数据的大小和类型、你想从数据中获得的见解以及如何使用这些见解。
机器学习算法分为分类、回归和聚类的图表。

以下是在监督和无监督机器学习之间进行选择的一些指导原则：
如果您需要训练模型进行预测，例如连续变量的未来值（如温度或股票价格）或分类（例如从网络摄像头视频片段中识别汽车制造商），请选择监督学习。
如果您需要探索数据并希望训练模型以找到良好的内部表示（例如将数据分成簇），请选择无监督学习。


# 分布式机器学习技术 sum:1
## 分裂学习
## 联邦学习
分裂学习 (Split Learning)

分裂学习是一种分布式机器学习方法，其中模型训练被分割成多个阶段，并在不同的计算节点上执行。这种设置通常用于处理数据隐私问题，因为它允许数据保留在本地设备上，而仅传输中间计算结果进行聚合。分裂学习的基本流程如下：

1. **数据分区**：原始数据集被分割成多个部分，每个部分保留在不同的设备或位置。
2. **模型分割**：神经网络或其他机器学习模型被分割成两个或更多的部分，每个部分在不同的计算节点上运行。
3. **前向传播**：输入数据在本地设备上通过模型的第一部分进行前向传播，生成中间表示。
4. **中间结果交换**：中间表示被发送到另一个计算节点，在那里它通过模型的剩余部分进行处理。
5. **反向传播**：误差通过模型的剩余部分反向传播，并返回到原始数据所在的节点。
6. **梯度更新**：原始节点根据接收到的梯度更新其模型参数。
7. **迭代**：上述过程在多个训练周期中重复，直到满足收敛条件。

分裂学习的一个常见应用是在客户端-服务器架构中，其中客户端拥有敏感数据，而服务器负责大部分计算工作。
联邦学习 (Federated Learning)

联邦学习是一种分布式机器学习技术，旨在解决数据隐私和数据孤岛问题。在这种框架下，多个客户端（如移动设备或传感器）持有自己的数据集，而无需将数据集中到一个中心服务器上。联邦学习的核心思想是让模型去到数据所在的地方，而不是将数据带到模型所在的地方。其基本流程如下：

1. **模型分发**：一个全局模型被分发到各个客户端。
2. **本地训练**：每个客户端使用本地数据集对模型进行训练，并计算梯度更新。
3. **更新聚合**：客户端将计算出的模型更新发送回中心服务器，但不共享原始数据。
4. **全局模型更新**：中心服务器聚合所有客户端的模型更新，并用这些更新来更新全局模型。
5. **迭代**：上述过程在多个训练轮次中重复，直到满足收敛条件。

联邦学习的关键优势在于它能够保护数据隐私，因为原始数据始终保留在客户端上，同时仍然能够协同训练一个高性能的机器学习模型。

总结来说，分裂学习和联邦学习都是为了在保护数据隐私的同时进行有效的机器学习。分裂学习通常涉及模型的分割并在不同节点上执行，而联邦学习则侧重于在本地设备上进行训练并将模型更新汇总到中心服务器。